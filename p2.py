# -*- coding: utf-8 -*-
"""Tulasi_MLP1_Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CZY2cEGtT24mVSP1I_Mj0Xo2zgfF5lgz

## Import dependencies
"""

# Import TensorFlow and TensorFlow Datasets

import tensorflow_datasets as tfds
import tensorflow as tf

import os
import time

print(tf.__version__)

learning_algorithm = "Adam" #"Adam"

# Download the dataset
#Download the MNIST dataset and load it from TensorFlow Datasets. This returns a dataset in tf.data format.
datasets, info = tfds.load(name='mnist', shuffle_files=True, with_info=True, as_supervised=True)

mnist_train, mnist_test = datasets['train'], datasets['test']

"""## Define distribution strategy
Create a MirroredStrategy object. This will handle distribution, and provides a context manager (tf.distribute.MirroredStrategy.scope) to build your model inside.
"""

#strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1"])
strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1", "/gpu:3"], cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
#strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1", "/gpu:2"])
#strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1"])
#strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0"])

print('Number of devices: {}'.format(strategy.num_replicas_in_sync))

"""## Setup input pipeline
When training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. In general, use the largest batch size that fits the GPU memory, and tune the learning rate accordingly.
"""

# You can also do info.splits.total_num_examples to get the total
# number of examples in the dataset.

num_train_examples = info.splits['train'].num_examples
num_test_examples = info.splits['test'].num_examples

BUFFER_SIZE = 10000

BATCH_SIZE_PER_REPLICA = 64
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

"""Pixel values, which are 0-255, have to be normalized to the 0-1 range. Define this scale in a function."""

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255

  return image, label
start_time = time.perf_counter()


"""Apply this function to the training and test data, shuffle the training data, and batch it for training. Notice we are also keeping an in-memory cache of the training data to improve performance."""

train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)

y_true = []
for image,label in eval_dataset:
  y_true = y_true + list(label)
  #print(image.shape, label.shape)
print(len(y_true))

"""# Create the model
Create and compile the Keras model in the context of strategy.scope.
"""

# Create the model
with strategy.scope():
  model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10)
])
  if learning_algorithm == "SGD":
    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.0),
                  metrics=['accuracy'])
  elif learning_algorithm == "Adam":
    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False
),
                  metrics=['accuracy'])

# Define the checkpoint directory to store the checkpoints

checkpoint_dir = './training_checkpoints'
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

# Function for decaying the learning rate.
# You can define any decay function you need.
def decay(epoch):
  if epoch < 3:
    return 1e-3
  elif epoch >= 3 and epoch < 7:
    return 1e-4
  else:
    return 1e-5

# Callback for printing the LR at the end of each epoch.
class PrintLR(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    print('\nLearning rate for epoch {} is {}'.format(epoch + 1,model.optimizer.lr.numpy()))

callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                       save_weights_only=True),
    tf.keras.callbacks.LearningRateScheduler(decay),
    PrintLR()
]

"""# Train and evaluate
Now, train the model in the usual way, calling fit on the model and passing in the dataset created at the beginning of the tutorial. This step is the same whether you are distributing the training or not.
"""

history = model.fit(train_dataset, validation_data=eval_dataset, epochs=30, callbacks=callbacks)
#history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)
#model.fit()

end_time = time.perf_counter()

print('elapsed time(s) is' , end_time-start_time)


from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

y_pred = model.predict_classes(eval_dataset)
accuracy = accuracy_score(y_true, y_pred)
print("Accuracy: {}".format(accuracy))
# confusion matrix
matrix = confusion_matrix(y_true, y_pred)
print(matrix)

import matplotlib.pyplot as plt
#history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)
# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
"""

"""
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#Export to SavedModel
#Export the graph and the variables to the platform-agnostic SavedModel format. After your model is saved, you can load it with or without the scope.
path = 'saved_model/'
model.save(path, save_format='tf')


#Load the model without strategy.scope.
unreplicated_model = tf.keras.models.load_model(path)

unreplicated_model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=['accuracy'])

eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)

print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))

#Load the model with strategy.scope.
with strategy.scope():
  replicated_model = tf.keras.models.load_model(path)
  replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                           optimizer=tf.keras.optimizers.Adam(),
                           metrics=['accuracy'])

  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)
  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))

#!tensorboard --logdir='/content/logs'

"""
keras.callbacks.TensorBoard(
    log_dir="/content/logs",
    histogram_freq=0,  # How often to log histogram visualizations
    embeddings_freq=0,  # How often to log embedding visualizations
    update_freq="epoch",
)  # How often to write logs (default: once per epoch)
"""